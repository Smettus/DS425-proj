{"cells":[{"cell_type":"markdown","source":["# Setup for a small text completion\n","If you want to use the GPU, select T4 as a hardware accelarator. This should allow you to run models on the GPU.\n","\n","Running 'nvidia-smi' in the terminal (that's why the exclamation mark is there) shows you the status of the GPU use. The free tier gives you around 15Gb of VRAM."],"metadata":{"id":"_mWiuAmP0yGM"}},{"cell_type":"code","source":["# added this to avoid running into locale issues with 'nvidia-smi'\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"],"metadata":{"id":"jEuIl16MJpWP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BKnQGSzu1aqa","executionInfo":{"status":"ok","timestamp":1710427943890,"user_tz":-60,"elapsed":316,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"99898045-30a7-454d-9874-a07802640af3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Mar 14 14:52:23 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   52C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"9iXFtuUu17_i"}},{"cell_type":"code","source":["# for data wrangling\n","import json\n","import pandas as pd\n","# for pretty printing\n","from IPython.display import Markdown"],"metadata":{"id":"B_eJ-mFzAkud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dependencies to work with HuggingFace models\n","!pip install auto-gptq accelerate optimum transformers"],"metadata":{"id":"W5bY-jQy19sZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710429592123,"user_tz":-60,"elapsed":90072,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"8511f807-3d0a-482d-f226-130eb89d7860"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/290.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n","Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"markdown","source":["# Zero-shot or Few-shot classification with a LLM"],"metadata":{"id":"g6luicdb2Woz"}},{"cell_type":"code","source":["# packages\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, pipeline\n","\n","# model path (cf. https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-code-ft-GPTQ)\n","model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n","\n","# setup\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","config = AutoConfig.from_pretrained(model_name_or_path)\n","model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", config=config)"],"metadata":{"id":"e6m4Grhs2QtW","colab":{"base_uri":"https://localhost:8080/","height":520},"executionInfo":{"status":"error","timestamp":1710429599130,"user_tz":-60,"elapsed":1227,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"1e26c8a9-03d7-4ae1-c4c3-5f3fa0251c8c"},"execution_count":3,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f3c900372acc>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2943\u001b[0m                 )\n\u001b[1;32m   2944\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2945\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2946\u001b[0m                     \u001b[0;34m\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2947\u001b[0m                 )\n","\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["### A very small example"],"metadata":{"id":"k0UXLn3J-aWl"}},{"cell_type":"code","source":["# define the promp structure and usage (few-shot classification)\n","exampleprompt=\"\"\"\n","<s>[INST] You are a helpful code assistant. Your task is to generate a valid JSON object based on the given information. So for instance the following:\n","name: John\n","lastname: Smith\n","address: #1 Samuel St.\n","would be converted to:[/INST]\n","{\n","\"address\": \"#1 Samuel St.\",\n","\"lastname\": \"Smith\",\n","\"name\": \"John\"\n","}\n","</s>\n","[INST]\n","name: Ted\n","lastname: Pot\n","address: #1 Bisson St.\n","[/INST]\"\"\"\n"],"metadata":{"id":"5jTXGzXV50Az"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode the input into tokens and put on GPU\n","inputtokens = tokenizer(exampleprompt, return_tensors=\"pt\").input_ids.cuda()\n","\n","inputtokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wj56DQD7Eir","executionInfo":{"status":"ok","timestamp":1710428199274,"user_tz":-60,"elapsed":372,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"458096c3-fbf3-4af5-9faf-b2bb2383b10b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[    1, 28705,    13,     1,   733, 16289, 28793,   995,   460,   264,\n","         10865,  2696, 13892, 28723,  3604,  3638,   349,   298,  8270,   264,\n","          3716,  9292,  1928,  2818,   356,   272,  2078,  1871, 28723,  1537,\n","           354,  3335,   272,  2296, 28747,    13,   861, 28747,  2215,    13,\n","          4081,   861, 28747,  6717,    13,  5540, 28747,   422, 28740, 16595,\n","           662, 28723,    13, 28727,   474,   347, 15514,   298, 28747, 28792,\n","         28748, 16289, 28793,    13, 28751,    13, 28739,  5540,  1264, 11441,\n","         28740, 16595,   662,  9191,    13, 28739,  4081,   861,  1264,   345,\n","         10259,   372,   548,    13, 28739,   861,  1264,   345, 14964, 28739,\n","            13, 28752,    13,     2, 28705,    13, 28792, 16289, 28793,    13,\n","           861, 28747, 15268,    13,  4081,   861, 28747, 10650,    13,  5540,\n","         28747,   422, 28740,   365, 20947,   662, 28723,    13, 28792, 28748,\n","         16289, 28793]], device='cuda:0')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# complete the input (this is the autoregressive generation part)\n","outputtokens = model.generate(inputtokens, max_new_tokens=512)\n","\n","outputtokens[0] # you can ignore the warning, the default values used are OK."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdGYHoFC7TPV","executionInfo":{"status":"ok","timestamp":1710428262979,"user_tz":-60,"elapsed":1426,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"9ef0a786-35a8-437a-b1f2-34bf168e4a91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([    1, 28705,    13,     1,   733, 16289, 28793,   995,   460,   264,\n","        10865,  2696, 13892, 28723,  3604,  3638,   349,   298,  8270,   264,\n","         3716,  9292,  1928,  2818,   356,   272,  2078,  1871, 28723,  1537,\n","          354,  3335,   272,  2296, 28747,    13,   861, 28747,  2215,    13,\n","         4081,   861, 28747,  6717,    13,  5540, 28747,   422, 28740, 16595,\n","          662, 28723,    13, 28727,   474,   347, 15514,   298, 28747, 28792,\n","        28748, 16289, 28793,    13, 28751,    13, 28739,  5540,  1264, 11441,\n","        28740, 16595,   662,  9191,    13, 28739,  4081,   861,  1264,   345,\n","        10259,   372,   548,    13, 28739,   861,  1264,   345, 14964, 28739,\n","           13, 28752,    13,     2, 28705,    13, 28792, 16289, 28793,    13,\n","          861, 28747, 15268,    13,  4081,   861, 28747, 10650,    13,  5540,\n","        28747,   422, 28740,   365, 20947,   662, 28723,    13, 28792, 28748,\n","        16289, 28793,   371,    13, 28739,  5540,  1264, 11441, 28740,   365,\n","        20947,   662,  9191,    13, 28739,  4081,   861,  1264,   345, 28753,\n","          322,   548,    13, 28739,   861,  1264,   345, 28738,   286, 28739,\n","           13, 28752,     2], device='cuda:0')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# this is the completion in tensor form, where whe stripped the input and the termination character\n","outputtokens[0][inputtokens.shape[1]:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uhsruPaK_KjY","executionInfo":{"status":"ok","timestamp":1710428265006,"user_tz":-60,"elapsed":351,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"43bf079d-0256-4ae8-e4ce-dba8fe93c2a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  371,    13, 28739,  5540,  1264, 11441, 28740,   365, 20947,   662,\n","         9191,    13, 28739,  4081,   861,  1264,   345, 28753,   322,   548,\n","           13, 28739,   861,  1264,   345, 28738,   286, 28739,    13, 28752,\n","            2], device='cuda:0')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# this is the result in text\n","result = tokenizer.decode(outputtokens[0][inputtokens.shape[1]:-1])\n","Markdown(result)"],"metadata":{"id":"_t-q82zA9p5I","colab":{"base_uri":"https://localhost:8080/","height":66},"executionInfo":{"status":"ok","timestamp":1710428414078,"user_tz":-60,"elapsed":351,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"7c4c7538-a1c8-43d2-a01b-31bb6d0645e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"{\n\"address\": \"#1 Bisson St.\",\n\"lastname\": \"Pot\",\n\"name\": \"Ted\"\n}"},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# this is the result parsed as json\n","json.loads(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9dehv5J-o2O","executionInfo":{"status":"ok","timestamp":1710428417091,"user_tz":-60,"elapsed":5,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"d1edfe8f-344e-41ca-fc0f-d4fb199f7937"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'address': '#1 Bisson St.', 'lastname': 'Pot', 'name': 'Ted'}"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### A more extensive example"],"metadata":{"id":"aIK0saMWA9PA"}},{"cell_type":"code","source":["from string import Template\n","\n","# General placeholder\n","CUSTOMPROMPT = \"\"\"<s>[INST] You are a clever analysist who detects the presence of the following aspects in texts:\n","- Animals\n","- Africa (in the broad sense)\n","- Call for a coup\n","\n","You only return and reply with valid, iterable RFC8259 compliant JSON in your responses.\n","You do NOT provide any additional information, only the JSON is returned.\n","\n","For example, the following texts\n","\"I like the presence of elephant in Botswana\"\n","\"The president of South-Africa should go!\"\n","\"Vladimir is treating his soldiers like dogs\"\n","would result in:[/INST]\n","{\"TEXT\": \"I like the presence of elephant in Botswana\", \"ANIMALS\": \"True\", \"AFRICA\": \"True\", \"COUP\", \"False\"}\n","{\"TEXT\": \"The president of South-Africa should go!\", \"ANIMALS\": \"False\", \"AFRICA\": \"True\", \"COUP\", \"True\"}\n","{\"TEXT\": \"Vladimir is treating his soldiers like dogs\", \"ANIMALS\": \"True\", \"AFRICA\": \"False\", \"COUP\", \"False\"}\n","</s>\n","[INST]\n","$content\n","[/INST]\n","\"\"\"\n","\n","# Making it into a template with \"$content\" as one of its values\n","PROMPT = Template(CUSTOMPROMPT)\n","\n","MESSAGE = \"The leaders of Niger and Ghana should be locked up like animals and their families slaughtered!\""],"metadata":{"id":"tpMkG0miFhsa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we can define a custom classification function doing all of this\n","def classifythis(text, model, tokenizer, **kwargs):\n","  # generate the content to encode\n","  msg = PROMPT.substitute(content=text)\n","  # load the input tokens on the same device as the model\n","  inputtokens = tokenizer(msg, return_tensors=\"pt\").to(model.device)\n","  # determine the input length\n","  inputlength = inputtokens[\"input_ids\"].shape[1]\n","  # generate the output\n","  outputtokens = model.generate(**inputtokens, max_new_tokens=120)\n","  # decode\n","  answer = tokenizer.decode(outputtokens[-1][inputlength:-1])\n","\n","  return answer\n","\n"],"metadata":{"id":"tbZRPsLnBMTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# single application\n","classifythis(MESSAGE, model, tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"Pq5INY06Arki","executionInfo":{"status":"ok","timestamp":1710428510879,"user_tz":-60,"elapsed":2340,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"3c7ee30e-57bd-4b5d-fcb4-ed4eee3d0d0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["'{\"TEXT\": \"The leaders of Niger and Ghana should be locked up like animals and their families slaughtered!\", \"ANIMALS\": \"True\", \"AFRICA\": \"True\", \"COUP\": \"True\"}'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["We can now apply our method on a set of texts to obtain a zero or few-shot classification and pour the results in a dataframe."],"metadata":{"id":"JncmPWCQJMpa"}},{"cell_type":"code","source":["messages = [\"The leaders of Niger and Ghana should be locked up like animals and their families slaughtered!\",\n","            \"All animals are created equal, but some are more equal than others\",\n","            \"Never gonna give you up, Never gonna let you down, Never gonna run around and desert you\",\n","            \"Ma mère m'a donné cent francs pout acheter un chien\",\n","            \"Ik ben die warmte hier beu\",\n","            \"jungle book was a great read, the kids loved it!\",\n","            \"\"\"группа \"Вагнер\" активно поддерживает повстанцев и дестабилизирует ситуацию в Сахельском регионе в целом.\"\"\",\n","            \"\"\"the Wagner group is actively supporting the rebels and destabilising the Sahel region as a whole.\"\"\"] # translation of the text above]\n","\n","res = pd.DataFrame(map(lambda t: json.loads(classifythis(t, model, tokenizer)), messages))"],"metadata":{"id":"t3eWfdNBI_b0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710428550266,"user_tz":-60,"elapsed":18903,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"d476e7f3-b1f0-46ac-fd13-319f317f5dc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]}]},{"cell_type":"code","source":["res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"QPS7qqF2KwSY","executionInfo":{"status":"ok","timestamp":1710428551754,"user_tz":-60,"elapsed":236,"user":{"displayName":"bart de clerck","userId":"14846407220675336706"}},"outputId":"711ef4b1-6bc2-4094-fc82-b41b6ac3fcae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                TEXT ANIMALS AFRICA   COUP\n","0  The leaders of Niger and Ghana should be locke...    True   True   True\n","1  All animals are created equal, but some are mo...    True  False  False\n","2  Never gonna give you up, Never gonna let you d...   False  False  False\n","3  Ma mère m'a donné cent francs pour acheter un ...    True  False  False\n","4                         Ik ben die warmte hier beu   False  False  False\n","5   jungle book was a great read, the kids loved it!    True  False  False\n","6  группа \"Вагнер\" активно поддерживает повстанце...   False   True  False\n","7  the Wagner group is actively supporting the re...   False   True  False"],"text/html":["\n","  <div id=\"df-c4f881c4-eba1-47cf-85ac-f0ba571f68ff\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>TEXT</th>\n","      <th>ANIMALS</th>\n","      <th>AFRICA</th>\n","      <th>COUP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The leaders of Niger and Ghana should be locke...</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>All animals are created equal, but some are mo...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Never gonna give you up, Never gonna let you d...</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ma mère m'a donné cent francs pour acheter un ...</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Ik ben die warmte hier beu</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>jungle book was a great read, the kids loved it!</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>группа \"Вагнер\" активно поддерживает повстанце...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>the Wagner group is actively supporting the re...</td>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4f881c4-eba1-47cf-85ac-f0ba571f68ff')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c4f881c4-eba1-47cf-85ac-f0ba571f68ff button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c4f881c4-eba1-47cf-85ac-f0ba571f68ff');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cfa46dc3-40f3-44d5-b076-a49b1dd1c4b7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfa46dc3-40f3-44d5-b076-a49b1dd1c4b7')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cfa46dc3-40f3-44d5-b076-a49b1dd1c4b7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"res","summary":"{\n  \"name\": \"res\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"TEXT\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"All animals are created equal, but some are more equal than others\",\n          \"jungle book was a great read, the kids loved it!\",\n          \"The leaders of Niger and Ghana should be locked up like animals and their families slaughtered!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ANIMALS\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"False\",\n          \"True\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AFRICA\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"False\",\n          \"True\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"COUP\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"False\",\n          \"True\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"Mj3i-Si2hCZo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For more advanced use-cases, consider LangChain: https://www.langchain.com/"],"metadata":{"id":"rVJkZWRti2_6"}},{"cell_type":"code","source":[],"metadata":{"id":"dEsuJvgkjADJ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"toc_visible":true,"provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1706793526416}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}