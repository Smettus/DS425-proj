{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os._exit(00)"
      ],
      "metadata": {
        "id": "k0cQQ8hm30y9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall accelerate transformers # https://stackoverflow.com/questions/76781329/importerror-with-transformers-and-accelerate-in-google-colab-seq2seqtrainingarg"
      ],
      "metadata": {
        "id": "xRA4gBjN-n2N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls /content/drive/MyDrive/AI/models"
      ],
      "metadata": {
        "id": "ldxqz6ZpA4mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate>=0.20.3 transformers\n",
        "# then restart runtime if it does not work later on in the AutoModelForCausalLM\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "7swmosa3-zkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes\n",
        "#!pip install accelerate\n",
        "!pip install transformers\n",
        "!pip install langchain\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "eqUofMf8xArG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install accelerate\n",
        "#!pip install huggingface_hub\n",
        "#pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "p3n8EUzq2KmW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kDpo1Mlkv9Nk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral LLM\n"
      ],
      "metadata": {
        "id": "kEq02Kzg-5eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/AI/models/mistral7B_4bit"
      ],
      "metadata": {
        "id": "rZWdMq0pPR0j",
        "outputId": "f52e623b-730a-471c-a885-ba04c74c0a4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\tmistral7B_4bit.pt  special_tokens_map.json  tokenizer.json\n",
            "generation_config.json\tmodel.safetensors  tokenizer_config.json    tokenizer.model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if already downloaded:\n",
        "path = \"/content/drive/MyDrive/AI/models/mistral7B_4bit/\"\n",
        "#torch.save(model_4bit.state_dict(), path)\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\")  # does its own .to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "bX_P_aD_ACOJ",
        "outputId": "ad5a89c5-77b1-4b68-9b58-d6c50b87748b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ed5538a5febc>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/AI/models/mistral7B_4bit/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#torch.save(model_4bit.state_dict(), path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_4bit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# does its own .to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3025\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;34m\"and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {model_4bit.get_memory_footprint() / 1e6:.2f} MB\")"
      ],
      "metadata": {
        "id": "cwTNOcqPDiKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the model quantization to be able to run in a free Google Colab notebook\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "# Version of the Mistral model used\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "# Model and tokenizer\n",
        "#model_4bit = AutoModelForCausalLM.from_pretrained( model_id, device_map=\"auto\",quantization_config=quantization_config)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# So input sequence in model, embedded in integers, to tokens. tokenizer splits up sentence\n",
        "# Model is no more than dictionary of tokens to integers. then inverse, integers get redone into tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "0Pv3kreiyDuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model & tokenizer\n",
        "#path = F\"/content/drive/MyDrive/AI/models/mistral7B_4bit\"\n",
        "#model_4bit.save_pretrained(path)\n",
        "#tokenizer.save_pretrained(path)"
      ],
      "metadata": {
        "id": "R_mbM3CSODG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Way to ask questions to the model:\n",
        "def askQuestion(prompt, max_new_tokens = 500, temperature = 0.8, top_p = 0.9, do_sample=True, repetition_penalty=1.2):\n",
        "  \"\"\" Encode the input prompt with Mistral instruct format, encode the prompt, use the Quantized Mistral model, decode the reponse and remove the input prompt from this response\n",
        "\n",
        "      Args:\n",
        "        prompt (str): Input prompt\n",
        "        max_new_tokens (int): Specifies the maximum number of new tokens (words or pieces of words) that the model is allowed to generate in response to the prompt.\n",
        "        temperature (float): Controls the randomness of the model's responses. Lower temperatures (closer to 0) make the model's outputs more deterministic and repetitive,\n",
        "                             while higher temperatures (closer to 1 or above) increase diversity and creativity in the responses.\n",
        "        top_p (float): Controls the model's sampling process, where it selects the next token based on a cumulative probability distribution of the possible next tokens.\n",
        "                       A common setting might be top_p = 0.9, which means the model will only consider the smallest set of tokens whose cumulative probability exceeds 90%\n",
        "                       for generating the next token. This approach helps to focus the model's choices on more likely outcomes, reducing the\n",
        "                       chance of selecting very improbable tokens and generally producing more coherent text.\n",
        "\n",
        "        do_sample (bool): The do_sample parameter controls whether the Large Language Model (LLM) generates text based on a sampling approach rather than simply\n",
        "                          choosing the most likely next token at each step. When do_sample is set to True, the model uses the probabilities of each token to randomly\n",
        "                          generate the next token in the sequence. This method introduces variability and creativity in the generated text, making the outputs less\n",
        "                          deterministic and more diverse.\n",
        "        repetition_penalty (float): is used to discourage or encourage the repetition of tokens (words or pieces of words) in the text generated by the model.\n",
        "                                   It modifies the likelihood of tokens based on their previous occurrences in the generated text,\n",
        "                                   aiming to control redundancy and improve the diversity and quality of the output.\n",
        "\n",
        "      Returns:\n",
        "        (str): Clean response to the prompt\n",
        "  \"\"\"\n",
        "  prompt = f\"\"\"[INST]{prompt}[/INST]\"\"\"\n",
        "  input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")  # should look online how tokenizer works\n",
        "  output = model_4bit.generate(input, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p, repetition_penalty=repetition_penalty, do_sample=do_sample)\n",
        "  answer = tokenizer.decode(output[0],skip_special_tokens=True).replace(prompt, \"\" )\n",
        "  return answer\n",
        "\n",
        "  # temperature = stochastic influence of generating next word"
      ],
      "metadata": {
        "id": "qwmNpKd-B0YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Asking Questions"
      ],
      "metadata": {
        "id": "SQESsjxjCan4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Enumerate the events between the invasion of ukraine by russia since 2022\"\n",
        "response = askQuestion(question,temperature = 0, top_p = 1.0, do_sample=False, repetition_penalty=0.05) # no creativity, almost totally deterministic\n",
        "print(response)"
      ],
      "metadata": {
        "id": "eUf-urJ3CZ4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More creative. parameters are\n",
        "question = \"Enumerate the events between the invasion of ukraine by russia since 2022. Put the dates in front.\"\n",
        "response = askQuestion(question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "7UB0I1dvDG0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other model, Google Gemma (open source Gemini)\n"
      ],
      "metadata": {
        "id": "0V5mmoMUEj_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ryX_MO3dEjxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eGDGJ6ZEbGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}